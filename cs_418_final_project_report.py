# -*- coding: utf-8 -*-
"""CS_418_Final_Project_Report.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_n-dxkIhNPhnuj6VKnOVi8phzDasPrem

# **Project Introduction**:

**PROJECT IDEA**

- Our big Idea is Natural disasters and their prediction. The problem here we want to solve is to reduce to the number of deaths that occur due to natural disaster by predicting the disaster beforehand.
As we know natural disaster account for a huge number of death count for humans all over the globe and with this project we want to figure out if there is a way in which we can predict the occurrence of this disasters so that we can increase the number of lives that can be saved by doing so.
The idea of the project came from the recent natural disasters in Turkey and Syria where a huge number of people died an untimely and horrific death.


**DATA**

- We Plan to use data on natural disasters and their geography of occurrence.

1. URL - https://public.emdat.be/
The size of the data is about  12470 rows and  50 columns.
It has all the information about all the disaster in he world from 1990 to present and their impact, location, economic loss due to disaster, and number of people affected due to the disaster.

2. URL - https://corgis-edu.github.io/corgis/csv/earthquakes/
The size of the data is about 8395 rows and 18 columns.
It has all the information about an earthquake, including the impact, location, and time.

**RESEARCH QUESTIONS**

- The first data science research question that we have is: **What is the potential number of lives that can be saved by accurately predicting an earthquake before it occurs worldwide?**

- The second data science research question that we have is: **What is the most likely latitudes and longitudes of the upcoming major earthquakes that will occur in the world?**

#**Data cleaning**: 
 

- In the below sections we have provided all the steps that we have carried out in order to clean the data.
"""

# The necessary Libraries required for the project.
import pandas as pd
import warnings
import matplotlib.pyplot as plt

"""# **Table 1 - Emdat Data file**

- The first step in the proceess is to read the Excel file and convert it into a pandas dataframe.
"""

# Using warnigns in order to catch and not display unnecessary warnings
with warnings.catch_warnings(record=True):
    warnings.simplefilter("always")
    # Taking the data set and creating a pandas dataframe.
    dataframe_Em_Dat = pd.read_excel('CS418EM-Dat_data.xlsx', engine='openpyxl')

"""- The next few steps is to arrange the data prior to cleaning it.
- In this step we drop the header section of the file that contains information about the data that is in the file. This data is in the top four rows of the file.
"""

dataframe_Em_Dat = dataframe_Em_Dat.drop(dataframe_Em_Dat.index[[0, 1, 2, 3, 4]])
dataframe_Em_Dat

"""- In this step we make the first row which contains the name for each column in to the header."""

dataframe_Em_Dat.columns = dataframe_Em_Dat.iloc[0]
dataframe_Em_Dat

"""- Here we drop the first row which is the duplication of the header and also reset the index."""

dataframe_Em_Dat = dataframe_Em_Dat.drop(dataframe_Em_Dat.index[[0]])
dataframe_Em_dat = dataframe_Em_Dat.reset_index(drop=True)
dataframe_Em_Dat

"""- In this steps we can see all the columns that are in the dataframe, we will not require all of them but in order to get a better understanding of the data we can see how each column is necessary to understand any disaster."""

dataframe_Em_Dat.columns

"""- Here are all the columns in the Table.
       Dis No, Year, Seq, Glide, Disaster Group, Disaster Subgroup,
       Disaster Type, Disaster Subtype, Disaster Subsubtype,
       Event Name, Country, ISO, Region, Continent, Location,
       Origin, Associated Dis, Associated Dis2, OFDA Response,
       Appeal, Declaration, AID Contribution (000 US$), Dis Mag Value,
       Dis Mag Scale, Latitude, Longitude, Local Time, River Basin,
       Start Year, Start Month, Start Day, End Year, End Month,
       End Day, Total Deaths, No Injured, No Affected, No Homeless,
       Total Affected, Reconstruction Costs (000 US$),
       Reconstruction Costs, Adjusted (000 US$),
       Insured Damages (000 US$), Insured Damages, Adjusted (000 US$),
       Total Damages (000 US$), Total Damages, Adjusted (000 US$), CPI,
       Adm Level, Admin1 Code, Admin2 Code, Geo Locations.

- In this step we will drop each column that is not required in the analysis and only keep Year, Disaster Type, Latitude, Longitude, Local Time, Total Deaths, No Injured, No Affected, No Homeless, Total Deaths.

- Here we will remove every other unnecessaary attribute as it will only add to space without having much contribution to the project. We have selected Year in order to keep track of the year, Disaster Type in order to differentiate between different disaster that are in the table, and Latitude, Longitude and Death count as these three are necessary columns in order to carry out the ML Tasks.
"""

dataframe_Em_Dat_N = dataframe_Em_Dat[['Year', 'Disaster Type', 'Latitude', 'Longitude', 'Total Deaths']]
dataframe_Em_Dat_N

"""- In this step we will drop every row that contains a NaN value for the Total Affected, Latitude, and Longitude."""

# In this we can see every value where there is an NaN in dataframe.
dataframe_Em_Dat_N.isna()

dataframe_Em_Dat_N = dataframe_Em_Dat_N.dropna(subset=['Latitude','Longitude', 'Total Deaths'])
dataframe_Em_Dat_N

"""- Here we will convert the string datatypes of year and total deaths to integer data type."""

dataframe_Em_Dat_N = dataframe_Em_Dat_N.astype({"Year":"int","Total Deaths":"int"})
dataframe_Em_Dat_N

"""- In the following step we will see what other disasters are in the dataset and we can see that apart from Earthquake, Volcanic activity, Flood, Storm, Landslide. We will need to make the dataset just focus on Earthquake in the next step."""

dataframe_Em_Dat_N['Disaster Type'].unique()

"""- Here we still have different types of disasters in the Disaster type but we only want to focus on the earthquakes so we will delete the rows with other disasters."""

dataframe_Em_Dat_Earthquake = dataframe_Em_Dat_N[dataframe_Em_Dat_N['Disaster Type'] == 'Earthquake']
dataframe_Em_Dat_Earthquake

"""- Now in the next two steps we will clean the latitude and longitude columns, i.e. we will make sure that the N, S, E, W attritributes in the columns are correct converted to float with -/+ signs determining the direction for both latitude and longitude."""

# Get the latitude values.
latitudes = dataframe_Em_Dat_Earthquake.iloc[:, 2]
new_latitudes = []

# Loop through each of the values in the latitude and clean it appropriately.
for i in range(0, len(latitudes)):
  value = latitudes.iloc[i]
  if 'S' in value:
      new_latitudes.append(float(value.replace('S', '').replace(' ', '').replace('°', '').replace('Â', '')) * -1)
  elif 'N' in value:
      new_latitudes.append(float(value.replace('N', '').replace(' ', '').replace('°', '').replace('Â', '')))
  else:
      new_latitudes.append((float(value)))
with warnings.catch_warnings(record=True):
    warnings.simplefilter("always")
    dataframe_Em_Dat_Earthquake['Latitude'] = new_latitudes

dataframe_Em_Dat_Earthquake

# Get the longitude values.
longitudes = dataframe_Em_Dat_Earthquake.iloc[:, 3]
new_longitudes = []

# Loop through each of the values in the longitude and clean it appropriately.
for i in range(0, len(longitudes)):
  value = longitudes.iloc[i]
  if 'W' in value:
      new_longitudes.append(float(value.replace('W', '').replace(' ', '').replace('°', '').replace('Â', '')) * -1)
  elif 'E' in value:
      new_longitudes.append(float(value.replace('E', '').replace(' ', '').replace('°', '').replace('Â', '')))
  else:
      new_longitudes.append((float(value)))
with warnings.catch_warnings(record=True):
    warnings.simplefilter("always")
    dataframe_Em_Dat_Earthquake['Longitude'] = new_longitudes

dataframe_Em_Dat_Earthquake

"""- In the next step we will plot the box plot for each of the necessary columns in the dataframe and analyse the data."""

# Plotting box plot for each of the necessary values.
fig, ax = plt.subplots(1, 3, figsize=(10, 10))
ax[2].boxplot(dataframe_Em_Dat_Earthquake['Latitude'])
ax[2].set_title('Latitude')
ax[1].boxplot(dataframe_Em_Dat_Earthquake['Longitude'])
ax[1].set_title('Longitude')
ax[0].boxplot(dataframe_Em_Dat_Earthquake['Total Deaths'])
ax[0].set_title('Total Deaths')
fig.tight_layout(pad=1)
plt.show()

"""- In the above boxplots we can see that the values of each of the boxplots indicate a well worse data set. In the case of total deaths we can see that none of the values are negative and every value is between the range of 0 to higher number of deaths recorded in an earthquake. So in this case we can say that all values are legitimate and none of the value can be considered outliers.

- In case of latitudes and longitudes each of the values are in the range of -360 to +360 and hence all the value in the boxplot are in these range, which makes all the values for latitude and longitude legitimate.
"""

dataframe_Em_Dat_Cleaned = dataframe_Em_Dat_Earthquake
dataframe_Em_Dat_Cleaned

"""- Here now we have completed the steps of data cleaning where we clean each and every necessary aspect of the data. From just keeping the necessary columns to cleaning every value in the latiude and longitude column. Apart fromt his also proving the legitimacy of each value.

- At the start of the Data cleaning for table one we had about 12470 rows × 50 columns, and at the end we have ended up having about 581 rows × 5 columns.

# **Table 2 - earthquake.csv file**

- Here in this dataset we will start by reading the data into a pandas series.
"""

df_Earthquakes = pd.read_csv('earthquakes.csv')
df_Earthquakes

"""- In the next step we will drop each row where we have found a NaN which will eventually be unnecessary for the data."""

df_Earthquakes.dropna(inplace=True)
df_Earthquakes

"""- In the final step we will only keep columns that are necessary for the analysis, i.e. Latitude, longitude, and Time."""

df_Earthquakes = df_Earthquakes[['location.latitude', 'location.longitude', 'time.full']]
# Using warnigns in order to catch and not display unnecessary warnings
with warnings.catch_warnings(record=True):
    warnings.simplefilter("always")
    df_Earthquakes['time.full'] = pd.to_datetime(df_Earthquakes['time.full'])
df_Earthquakes

# Plotting the boxplots which shows the outliers in latitude and longitude
fig, ax = plt.subplots(1, 2, figsize=(10, 5))
ax[0].boxplot(df_Earthquakes['location.latitude'])
ax[0].set_title('Latitude')
ax[1].boxplot(df_Earthquakes['location.longitude'])
ax[1].set_title('Longitude')
plt.show()

# Removing the outliers from latitude and longitude columns

Q1 = df_Earthquakes['location.latitude'].quantile(0.25)
Q3 = df_Earthquakes['location.latitude'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
df_Earthquakes = df_Earthquakes[(df_Earthquakes['location.latitude'] > lower_bound) & (df_Earthquakes['location.latitude'] < upper_bound)]

Q1_2 = df_Earthquakes['location.longitude'].quantile(0.25)
Q3_2 = df_Earthquakes['location.longitude'].quantile(0.75)
IQR_2 = Q3_2 - Q1_2
lower_bound_2 = Q1_2 - 1.5 * IQR_2
upper_bound_2 = Q3_2 + 1.5 * IQR_2
df_Earthquakes = df_Earthquakes[(df_Earthquakes['location.longitude'] > lower_bound_2) & (df_Earthquakes['location.longitude'] < upper_bound_2)]

# Plotting the boxplots for latitude and longitude to see if all outliers were removed

fig, ax = plt.subplots(1, 2, figsize=(10, 5))
ax[0].boxplot(df_Earthquakes['location.latitude'])
ax[0].set_title('Latitude')
ax[1].boxplot(df_Earthquakes['location.longitude'])
ax[1].set_title('Longitude')
plt.show()

# Seeing the final data after cleaning
df_Earthquakes

"""- Here we can see that cleaning the data we have reduced the number of columns to 3 columns from a huge number of 18 columns. This 3 columns contains all the necessary information that will be required for the analysis of the problem statement. The number of rows reduced from 8394 to 7420. So we lost 974 rows in total while removing the outliers.

- We have also only kept the columns laongitude, latitude and time that will be helpful in the project in order to carry out the ML-Taks.

**Scaling**
- Our latitude and longitude data are already on the same scale and units, which means that we don't need to normalize them.
- The number of deaths, which is our target variable, is also on a similar scale across different earthquake events. Therefore, we may not need to scale this variable either.
- Both KNN and random forest regression algorithms are relatively insensitive to feature scaling. KNN relies on calculating distances between data points, which will be unaffected by scaling. Random forest is an ensemble of decision trees, where each tree is built on a random subset of features. The algorithm will automatically adjust for differences in scale by selecting the most relevant features.

Hence from all the above point there was no reason for us to scale the data in order to perform ML tasks.

#**Exploratory data analysis**: 
**Explain what your data looks like (words are fine, but visualizations are often better). Include a description of each table, the statistical data type of each column, the number of rows, etc. All the columns should be analyzed. You should try to find correlations between the different attributes. Include any interesting issues or preliminary conclusions you have about your data. All the tables should be analyzed. Please include the author of the EDA per table, at least one EDA per team member.**


-> **Author: Krish Patel - Table 1**
- After achieveing the final state of cleaning the data the table 1 which contains the EmDat data for natural disaster only contains the information that is absolutely necesary for carrying the analysis required for the problem statement. This Data contains 581 rows and 5 columns. Every column when analyzed with the help of .dtype we can see that each column contains object as the datatype. In this table we can see a correlation between latiude and longitude as they describe the geographic location of the natural disaster, apart from this the columns describing the number of people died, homeless, affected and injured are also correlated as they describe the total number of people that were affected due to the natural disaster which will be crucial in order to predict the number of that deaths that will occur due o a natural diasater like Earthquake.
"""

# Here we can see the dtypes of each and every column in the dataframe.
dataframe_Em_Dat_N.dtypes

"""-> **Author: Panth Patel - Table 2** 
- After cleaning the data and keeping only the columns that are required to predict the most likely latitudes and longitudes of the upcoming major earthquakes that will occur in the world, there are 3 columns with 8394 rows each. The columns latitude and longitude are of float data type and the time.full column is of datetime datatype. The columns latitude and longitude are correlated to each other as they give the location of the earthquake and time column gives the exact date and time of that particular earthquake. The date and time column goes from 27 July 2016 to 25 August 2016.An interesting issue which I see is that this table is for a ceratin period and cannot really say if the predictions would be accurate or not.
"""

# Here we can see the dtypes of each and every column in the dataframe.
df_Earthquakes.dtypes

"""#**Visualizations for EDA**: 
**That test an interesting hypothesis, along with an explanation about why you thought this was an interesting hypothesis to investigate. At least one visualization per team member.**

-> **Panth Patel - Table 2** 
- One interesting hypothesis to investigate based on the given EDA would be to check if there is a relationship between the latitude/longitude of the earthquake epicenters and the time of day when they occur. This scatter plot will show if there is any trend or pattern in the occurrence of earthquakes based on the time of day. If there is a noticeable pattern, it could suggest that certain times of day are more likely to have earthquakes and could be useful for predicting upcoming earthquakes. If there is no pattern, it could suggest that the time of day is not a useful predictor for earthquake occurrence.
"""

# Using warnigns in order to catch and not display unnecessary warnings
with warnings.catch_warnings(record=True):
    warnings.simplefilter("always")
    df_Earthquakes['hour'] = df_Earthquakes['time.full'].dt.hour
plt.scatter(df_Earthquakes['hour'], df_Earthquakes['location.longitude'], s=10)
plt.xlabel('Hour of day')
plt.ylabel('Longitude')
plt.title('Relationship between earthquake longitude and time of day')
plt.show()

"""As we can see that there is no relation between the time of the day and longitude. However we can notice that most earthquakes took place between longitudes -110 to -180."""

# Using warnigns in order to catch and not display unnecessary warnings
with warnings.catch_warnings(record=True):
    warnings.simplefilter("always")
    df_Earthquakes['hour'] = df_Earthquakes['time.full'].dt.hour
plt.scatter(df_Earthquakes['hour'], df_Earthquakes['location.latitude'], s=10)
plt.xlabel('Hour of day')
plt.ylabel('Latitude')
plt.title('Relationship between earthquake latitude and time of day')
plt.show()

"""As we can see that there is no relation between the time of the day and longitude. However we can notice that most earthquakes took place between longitudes 30 to 70."""

import folium

# Displaying the region with most number of earthquakes in the Data
m = folium.Map(location=[50, -140], zoom_start=2)
bounds = [[30, -180], [70, -110]]
folium.Rectangle(bounds, color='red', fill_opacity=0.1).add_to(m)
m

"""The latitude and longitude values that we got where most of the earthquakes took place form a large region that encompasses parts of North America, and Asia, as well as the Arctic Ocean

-> **Krish Patel - Table 1**
- Here we cann see that the hypithesis that will tend to be an interesting one would be, The frequency and severity of natural disasters have increased over time, resulting in a higher number of people affected each year. Now as we can see in the below graph that the number of deaths ten to be more in the years between 2010 and 2015 and decreases between 2015 to 2020. In this two senerios we can see from the visualization that the hypothesis is false and hence the visualization of total affected vs the year tend to be a useful tool in explaining this.
"""

# Create a bar plot of Total Affected vs Year
plt.bar(dataframe_Em_Dat_N['Year'], dataframe_Em_Dat_N['Total Deaths'])

# Set the title and axes labels
plt.title('Total Deaths vs Year')
plt.xlabel('Year')
plt.ylabel('Total Deaths')

# Show the plot
plt.show()

"""#**Model planning**: 
**Write down the ML models that you plan to use to answer the DS research questions and explain why these models make sense. For each question, propose at least three ML models and analyze their pros and cons. At least three models are proposed per team member.  
Source: https://sherbold.github.io/intro-to-data-science**

**For the research question : What is the most likely latitudes and longitudes of the upcoming major earthquakes that will occur in the world?**

Following are the 3 ML models we propose.

- ARMA - ARMA is a time series analysis technique that we can use to model and predict future values based on historical data. That is exactly what we are planning to do using the past earthquake data and predicting the future earthquake locations. One pro of ARMA is its ability to handle time series data and predict future values based on historical patterns. One con is that it assumes that the time series data is stationary, which may not be the case for all earthquake data. Another con is that it is not well-suited for handling non-linear relationships between the features and target variable.

- K-Means Clustering - We can use K-means to group earthquakes based on their location. One pro of K-Means clustering is that we can use it to identify natural groupings in the data without requiring prior knowledge of the labels. Which is also useful for identifying locations(latitude,longitude) that are more prone to earthquakes. One con is that it is sensitive to the choice of K value, which may not be known in advance.

- Random Forest Regression - We could use Random Forest for both classification and regression tasks. We can use it to predict the occurrence of an earthquake in a particular location based on historical earthquake data. One pro of Random Forest Classification is its ability to handle non-linear relationships between the features and target variable. One con is that it can be prone to overfitting if the number of trees is too large or if the trees are too deep.
"""

df_Earthquakes

"""- Here for predicting the latitudes and longitudes from the time, we are using Random forect regression."""

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error


# Extract year, month, day, and hour features from time.full column
df_Earthquakes['year'] = df_Earthquakes['time.full'].dt.year
df_Earthquakes['month'] = df_Earthquakes['time.full'].dt.month
df_Earthquakes['day'] = df_Earthquakes['time.full'].dt.day
df_Earthquakes['hour'] = df_Earthquakes['time.full'].dt.hour

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df_Earthquakes[['year', 'month', 'day', 'hour']], df_Earthquakes[['location.latitude', 'location.longitude']], test_size=0.2, random_state=42)

# Train Random Forest Regression model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Create a DataFrame with all days of May 2023
date_rng = pd.date_range(start='5/1/2023', end='5/31/2023', freq='D')
new_data = pd.DataFrame(date_rng, columns=['time.full'])
new_data['year'] = new_data['time.full'].dt.year
new_data['month'] = new_data['time.full'].dt.month
new_data['day'] = new_data['time.full'].dt.day
new_data['hour'] = new_data['time.full'].dt.hour

# Predict the locations for each day of May 2023
predicted_locations = rf_model.predict(new_data.drop(columns=['time.full']))
predicted_data = pd.DataFrame(predicted_locations, columns=['latitude', 'longitude'])
predicted_data.index = new_data['time.full']

predicted_data

# create a map centered on a specific location
m2 = folium.Map(location=[0, 0], zoom_start=2)

# add markers for each point in the dataframe
for index, row in predicted_data.iterrows():
    folium.Marker(location=[row['latitude'], row['longitude']]).add_to(m2)

# display the map
m2

"""**For the research question : What is the potential number of lives that can be saved by accurately predicting an earthquake before it occurs worldwide?**

Following are the 3 ML models we propose.

- K-Nearest Neighbor Clustering - KNN clustering can be used to put data points into groups based on similarities. We could use it to find groups of prior earthquakes that have shared locations and fatality counts in the context of earthquake prediction. Based on the location of future earthquakes, this information can be used by us to estimate how many people will die in each one. One pro of KNN clustering is that it is straightforward, understandable approach that can handle high-dimensional data. One con is that it may not be efficient if the underlying data has a complicated structure, and the number of clusters chosen can be arbitrary.

- Linear Regression - Linear regression may be used to simulate the connection between a dependent variable and one or more independent variables. We can model the association between the location of earthquakes and the quantity of fatalities using linear regression for death prediction. We can use the model once it has been trained on past earthquake data. One pro of linear regression is that it can handle both continuous and categorical independent variables and is a straightforward and understandable model. One con is that it relies on the independent and dependent variables having a linear connection, hence it may not work well when there is a non-linear relationship.

- Association Rule Mining - Association rule mining can be used to find patterns in data. We can use it to identify rules that describe the relationship between the location of earthquakes and the number of deaths. We can use these rules to predict the number of deaths in future earthquakes based on their location. One pro of association rule mining is that it can identify complex relationships between variables, and can handle both continuous and categorical data. One con is that it can be sensitive to noise in the data, and may generate a large number of rules that are difficult to interpret.
"""

dataframe_Em_Dat_Cleaned

"""- Here for predicting the number of deaths that can occur at a particular location with latitude and longitude, we are using KNN Clustering."""

# Import necessary libraries.
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics

previous_max_accuracy = 0
best_k_val = 0

# Craete data for everything.
data_x = dataframe_Em_Dat_Cleaned.iloc[:, 2:4]
data_y = dataframe_Em_Dat_Cleaned.iloc[:, 4]

# Create train test split.
data_x_train, data_x_test, data_y_train, data_y_test = train_test_split(data_x, data_y, test_size=0.2)

# Loop to get the best k-value.
for n in range(1, 50):

  # Create a model.
  knn_model = KNeighborsClassifier(n_neighbors=n, weights='distance')

  # Create a fot for the model.
  knn_model.fit(data_x_train, data_y_train)

  # Create a predictiong for the model.
  y_predicted = knn_model.predict(data_x_test)

  # Get the accuracy score.
  accuracy = metrics.accuracy_score(y_predicted, data_y_test)

  # Swap if maximum.
  if previous_max_accuracy < accuracy:
    previous_max_accuracy = accuracy
    best_k_val = n

# Now print the best accuarcy and k_val.
print('Best Accuracy:', previous_max_accuracy, f'at k={best_k_val}')

# Now calculate the predicted value from Table 2 using the best k_val.
test_x_data = predicted_data.iloc[:, 0:2]
test_x_data.columns = ['Latitude', 'Longitude']

# Create a model.
knn_model = KNeighborsClassifier(n_neighbors=best_k_val, weights='distance')

# Create a fot for the model.
knn_model.fit(data_x_train, data_y_train)

# Create a predictiong for the model.
y_test_prediction = knn_model.predict(test_x_data)
y_test_prediction

# Read in the data
latitude = test_x_data['Latitude'].values
longitude = test_x_data['Longitude'].values
death_counts = y_test_prediction

# Create the plot
plt.figure(figsize=(10, 8))
plt.scatter(longitude, latitude, s=death_counts*10, alpha=0.5, c='r')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Actual Deaths by Location')

# Show the plot
plt.show()

# Create the map
m2 = folium.Map(location=[0, 0], zoom_start=2)

# Add markers with popups for each point in the dataframe
for i in range(0, len(y_test_prediction)):
    popup_text = f"Predicted Deaths: {y_test_prediction[i]}"
    folium.Marker(location=[latitude[i], longitude[i]], popup=popup_text).add_to(m2)

# Display the map
m2

"""#**RESULT**: 
**Fully explain and analyze the results from your data, i.e. the inferences or correlations you uncovered, the tools you built, or the visualizations you created.**

- So accoridng to the data analaysis that we have carried out with the ML Models. Here are the keyt results that we have obtained.

**First Model(Random Forest Regression):** 
- In the first model we planned to extract the latitudes and longitudes that were likely to get hit with an earthquake in the month of may. And as you can see we able to sucessfully get the values from the Random forest regressor. 
- We were able to get about 31 values form the regressor which were the altitudes and longitudes of the earthquakes. 
- Most of this earthquakes when plotted on the map shows that the location where on the western cost of United States and some were in the waters of the Pacific Ocean. 
- The results does match with the real world as the western cost of the United States is one of the most active tectonique zone, which has been an epicenter of many earthquakes since a long time.

**Second Model(KNN Classifier):**
- After training the data on the training set from the Em-Dat data set and selecting a k value that produces the most accuracy we were able to use to latitudes and longitudes form the firast model in order to predict the deaths that can occur in those locations.
- After feeding the KNN with the predicted latitudes and longitudes we were able to gather deaths count that can occur at these locations. And most of the values that were predicted where 1's and 2's as the model did no take into account the population density at these locations.
- Once we were able to extract the data we plotted the data into a folium map which is interactive and once you click at the predicted location it gives you the marker with predicted death count at this location.

#**References**: 
**List all the resources we have used.**

- https://sherbold.github.io/intro-to-data-science

- https://regenerativetoday.com/learn-to-formulate-good-research-question-for-efficient-statistical-analysis/


- https://corgis-edu.github.io/corgis/csv/earthquakes/

- https://public.emdat.be/

- https://medium.com/analytics-vidhya/ml-algorithms-pros-cons-and-suitable-usages-b377c3c09f1b

- https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html

- https://www.datacamp.com/tutorial/k-nearest-neighbor-classification-scikit-learn

- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html

- https://docs.google.com/presentation/d/1FzOIvXhKBhWwsZ0Hi6-jnYn0SYEYdVop/edit#slide=id.p1

- https://pypi.org/project/folium/
"""